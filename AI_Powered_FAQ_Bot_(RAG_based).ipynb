{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY2ViTJyz9+brI5t/g4vYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhlemAmmar/AI-Powered-FAQ-Bot-RAG-based-/blob/main/AI_Powered_FAQ_Bot_(RAG_based).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **âš¡ RAG (Retrieval-Augmented Generation) minimal implementation**\n",
        "\n",
        "\n",
        "A RAG application combines retrieval and generation to give accurate, context-based answers.\n",
        "\n",
        "ğŸ”¹ Workflow\n",
        "\n",
        "\n",
        "\n",
        "1.   **Indexing**\n",
        "  * **Load**: Import data with Document Loaders.\n",
        "\n",
        "  * **Split**: Break documents into smaller chunks.\n",
        "\n",
        "   * **Store**: Save chunks in a Vector Store using embeddings.\n",
        "2.   **Retrieval & Generation**\n",
        "* **Retrieve**: Fetch relevant chunks for a user query.\n",
        "\n",
        "* **Generate**: An LLM creates an answer using the query + retrieved data."
      ],
      "metadata": {
        "id": "lz_yfFiX68YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lNLCWrX5AXW",
        "outputId": "41f7ad0a-5ae3-4511-c352-214e96fb0439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zuBjrGnU8g5y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a08b9ede"
      },
      "source": [
        "### ğŸ”— LangChain\n",
        "\n",
        "LangChain is a framework that makes it easier to build applications powered by Large Language Models (LLMs).  \n",
        "It connects LLMs with external data sources and tools, enabling **context-aware** and more powerful AI apps.\n",
        "\n",
        "---\n",
        " âœ¨ **Key Features**\n",
        "- **Components** â†’ Abstractions for LLMs, retrievers, parsers, etc.  \n",
        "- **Chains** â†’ Combine components into sequences or graphs for complex workflows.  \n",
        "- **Agents** â†’ Let LLMs interact with their environment and decide actions.  \n",
        "- **Indexing** â†’ Load, structure, and query external data.  \n",
        "- **LangServe** â†’ Deploy LangChain apps as APIs.  \n",
        "\n",
        "ğŸ“– [Learn more in the docs](https://python.langchain.com/docs/introduction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ› ï¸ **LangSmith**\n",
        "\n",
        "**LangSmith** is a platform for building **production-grade LLM applications**.  \n",
        "It allows you to **monitor, evaluate, and debug** your applications so you can ship faster and with confidence.  \n",
        "\n",
        "---\n",
        "\n",
        "âœ¨ **Key Capabilities**\n",
        "- **Monitoring** â†’ Track application performance in real time.  \n",
        "- **Evaluation** â†’ Assess outputs for quality and reliability.  \n",
        "- **Debugging** â†’ Inspect inputs, outputs, and intermediate steps.  \n",
        "- **Optimization** â†’ Continuously improve your LLM pipelines.  \n",
        "\n",
        "ğŸ“– [Learn more in the docs](https://docs.langchain.com/langsmith/home)\n"
      ],
      "metadata": {
        "id": "oCAqzXOeH4Yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "PqoYhDBruNij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"]=userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "l5PWWoh5H34I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. ***chat model: Google Gemini***\n",
        "\n"
      ],
      "metadata": {
        "id": "3zN2gb8VsZxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \"langchain[google-genai]\""
      ],
      "metadata": {
        "id": "xr1jR4lnHHdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74af913b-9eae-492e-e8d7-1e00686207ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/50.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.7/1.4 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "X0rSg9UkoxbM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "# chat model\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
      ],
      "metadata": {
        "id": "JKGcnp43ptju"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. ***embeddings model: OpenAI***"
      ],
      "metadata": {
        "id": "iqsKqLLfskvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "#embeddings model:\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
      ],
      "metadata": {
        "id": "q5LCMHf8qCbE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ***vector store: In-memory***"
      ],
      "metadata": {
        "id": "7n8plLY6sud7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-core"
      ],
      "metadata": {
        "id": "2xTirwlbsxoj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "#vector store\n",
        "vector_store = InMemoryVectorStore(embeddings)"
      ],
      "metadata": {
        "id": "s_omRZietgFY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG chain"
      ],
      "metadata": {
        "id": "tS9mt4NjucrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import START, StateGraph\n",
        "from typing_extensions import List, TypedDict"
      ],
      "metadata": {
        "id": "9iuM7OMPujUu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load and chunk contents of the blog\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "Q026MmMQtpDJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Index chunks\n",
        "_ = vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "id": "UnZY0dY9uoL8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt for question-answering\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")"
      ],
      "metadata": {
        "id": "7MIIqdFzwG8Q"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define state for application\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Define application steps\n",
        "\n",
        "def retrieve(state: State):\n",
        "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
        "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
        "    response = llm.invoke(messages)\n",
        "    return {\"answer\": response.content}\n",
        "\n"
      ],
      "metadata": {
        "id": "SE8pG8IxuA9b"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compile application and test\n",
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "fXH9yP0IwRQ5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = graph.invoke({\"question\": \"What is spaghetti?\"})\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VERthYEet1k3",
        "outputId": "1c33a3c6-b4f8-4ea6-eda8-0e61bc80cfae"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know the answer. The provided context discusses types of human memory and best practices for code commenting, but it does not contain any information about spaghetti.\n"
          ]
        }
      ]
    }
  ]
}